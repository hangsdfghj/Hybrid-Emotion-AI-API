import os
# é—œéµå„ªåŒ–ï¼šæŠ‘åˆ¶ TensorFlow çš„å•Ÿå‹•æ—¥èªŒå’Œè­¦å‘Š
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
os.environ["CUDA_VISIBLE_DEVICES"] = "-1" # ç¢ºä¿ä½¿ç”¨ CPU

import numpy as np
import jieba
# ç‚ºäº†é¿å…è¼‰å…¥éŒ¯èª¤ï¼Œæˆ‘å€‘åªåœ¨å¿…è¦æ™‚å°å…¥é€™äº›
try:
    import tensorflow as tf
    from tensorflow.keras.models import load_model
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    from tensorflow.keras.preprocessing.text import Tokenizer
    import pandas as pd
except ImportError as e:
    # å¦‚æœæ·±åº¦å­¸ç¿’åº«å®‰è£å¤±æ•—ï¼Œå°‡æœƒåœ¨é€™è£¡æ•ç²
    print(f"FATAL ERROR: ç„¡æ³•å°å…¥æ·±åº¦å­¸ç¿’åº« ({e})ã€‚è«‹æª¢æŸ¥ requirements.txt!")
    tf = None # å°‡é€™äº›æ¨¡çµ„è¨­ç‚º None ä»¥é¿å…å¾ŒçºŒå´©æ½°

from google import genai
from google.genai.errors import APIError
from flask import Flask, request, jsonify
from flask_cors import CORS

# --- 0. å…¨åŸŸè®Šæ•¸èˆ‡æ¨¡å‹è¼‰å…¥ (åƒ…åœ¨æœå‹™å•Ÿå‹•æ™‚åŸ·è¡Œä¸€æ¬¡) ---

# ğŸš¨ å‹™å¿…åœ¨ Render ç’°å¢ƒè®Šæ•¸ä¸­è¨­å®š GEMINI_API_KEY
client = genai.Client()

emotion_classes = np.array(['å­æƒ¡', 'å–œæ‚…', 'å¹³éœ', 'æ‚²å‚·', 'æ†¤æ€’', 'æœŸå¾…', 'ç„¦æ…®', 'é©šè¨']) 
max_len = 16 

# åˆå§‹åŒ– tokenizer å’Œ model è®Šæ•¸
tokenizer = None
final_model = None

# ğŸš¨ æ¨¡å‹è¼‰å…¥æ—è·¯ (Mock Model) - ä¿æŒä¸è®Šï¼Œä»¥ç¢ºä¿å•Ÿå‹•é€Ÿåº¦
class MockEmotionModel:
    """ç”¨æ–¼å–ä»£ TensorFlow æ¨¡å‹ï¼Œè®“æœå‹™å¿«é€Ÿå•Ÿå‹•ä¸¦æ¨¡æ“¬ä¸€å€‹é æ¸¬çµæœ (ä¾‹å¦‚: ç„¦æ…®)ã€‚"""
    def predict(self, padded_sequence, verbose=0):
        # æ¨¡æ“¬ä¸€å€‹ 'ç„¦æ…®' (ç´¢å¼• 6) çš„é«˜ä¿¡å¿ƒåº¦çµæœ
        return np.array([[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.65, 0.05]]) 

final_model = MockEmotionModel() 
print("æ¨¡å‹è¼‰å…¥å·²æ—è·¯ã€‚æ­£åœ¨ä½¿ç”¨æ¨¡æ“¬æ¨¡å‹é€²è¡Œå•Ÿå‹•æ¸¬è©¦ã€‚")


# Tokenizer é‡å»ºé‚è¼¯
print("--- æ­£åœ¨å˜—è©¦é‡å»º Tokenizer... ---")
if tf is not None and pd is not None and Tokenizer is not None:
    try:
        # è¼‰å…¥æ•¸æ“šç”¨æ–¼é‡å»º Tokenizer (å‡è¨­ emotion_data.csv å­˜åœ¨æ–¼æ ¹ç›®éŒ„)
        df = pd.read_csv('emotion_data.csv', header=None, names=['text', 'emotion'])
        
        # é‡æ–°åŸ·è¡Œåˆ†è©èˆ‡å»ºç«‹
        df['tokens'] = df['text'].apply(lambda x: list(jieba.cut(x, cut_all=False)))
        texts = [" ".join(tokens) for tokens in df['tokens']]
        
        # é‡æ–°å»ºç«‹ Tokenizer è®Šæ•¸
        tokenizer = Tokenizer(num_words=5000, oov_token="<unk>") 
        tokenizer.fit_on_texts(texts)
        
        print("'tokenizer' å·²æˆåŠŸå¾ emotion_data.csv é‡å»ºï¼")

    except FileNotFoundError:
        print("CRITICAL ERROR: ç„¡æ³•æ‰¾åˆ° 'emotion_data.csv' æª”æ¡ˆã€‚Tokenizer å»ºç«‹å¤±æ•—ã€‚")
        tokenizer = 'FILE_NOT_FOUND' # è¨­ç½®ç‚ºéŒ¯èª¤æ¨™èªŒ
    except Exception as e:
        print(f"CRITICAL ERROR: Tokenizer å»ºç«‹å¤±æ•—ï¼ŒåŸå› : {e}")
        tokenizer = 'BUILD_FAILED' # è¨­ç½®ç‚ºéŒ¯èª¤æ¨™èªŒ
else:
    print("CRITICAL ERROR: ç”±æ–¼å‡½å¼åº«å°å…¥å¤±æ•—ï¼ŒTokenizer ç„¡æ³•å»ºç«‹ã€‚")
    tokenizer = 'LIB_FAILED'


# --- 1. æ ¸å¿ƒé‚è¼¯å‡½å¼ ---

# 1.1 LSTM åˆ¤æ–·æƒ…ç·’ (ç¾åœ¨æœƒä½¿ç”¨ MockModel.predict)
def predict_emotion(text_input, model, tokenizer, max_len, emotion_classes):
    """ä½¿ç”¨ Mock æ¨¡å‹é æ¸¬è¼¸å…¥æ–‡æœ¬çš„æƒ…ç·’ï¼Œä»¥ç¹é TF è¼‰å…¥å•é¡Œã€‚"""
    
    if isinstance(tokenizer, str):
         # å¦‚æœ Tokenizer æ˜¯ä¸€å€‹éŒ¯èª¤æ¨™èªŒï¼ˆå­—ä¸²ï¼‰ï¼Œç›´æ¥è¿”å›éŒ¯èª¤
         return 'Tokenizer å¤±æ•—', 0.0
         
    # é€™è£¡çš„åˆ†è©å’Œ Padding ä»ç„¶æ˜¯å¿…è¦çš„æ­¥é©Ÿ
    tokens = list(jieba.cut(text_input, cut_all=False))
    text_processed = [" ".join(tokens)]
    
    # é€™è£¡å¯èƒ½å› ç‚º tokenizer è¼‰å…¥ä¸å…¨è€Œå´©æ½°
    try:
        sequence = tokenizer.texts_to_sequences(text_processed)
        padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post', truncating='post')
    except Exception as e:
        # å¦‚æœ Tokenizer èªæ³•ç´šå´©æ½°
        return f'Tokenizer è™•ç†å´©æ½°: {str(e)[:50]}...', 0.0
    
    # å‘¼å« MockModel.predict
    predictions = model.predict(padded_sequence, verbose=0)
    
    predicted_class = np.argmax(predictions, axis=1)[0]
    predicted_emotion = emotion_classes[predicted_class]
    confidence = predictions[0][predicted_class]
    
    return predicted_emotion, confidence

# 1.2 æ¨è–¦é‚è¼¯ (ä¿æŒä¸è®Š)
recommendation_logic = {
    'å–œæ‚…': {'type': 'ç¤¾äº¤å‹èˆˆè¶£ / åˆ†äº«', 'reason': 'ä½ å¿ƒæƒ…è¶…æ£’ï¼æ˜¯æ™‚å€™è·Ÿæœ‹å‹åˆ†äº«é€™ä»½å–œæ‚…ï¼Œèˆ‰è¾¦ä¸€å ´ç¾é£Ÿèšæœƒå§ï¼'},
    'æ‚²å‚·': {'type': 'å‰µé€ å‹ / ç™¼æ´©å‹èˆˆè¶£', 'reason': 'ä½ ç¾åœ¨çš„æƒ…ç·’éœ€è¦å‡ºå£ï¼Œä¸å¦‚æ‹¿èµ·ç´™ç­†ç•«ä¸‹ä½ çš„å¿ƒæƒ…ï¼Œæˆ–å¯«é»æ±è¥¿å§ï¼è®“å‰µä½œæ›¿ä½ èªªå‡ºé‚£äº›èªªä¸å‡ºå£çš„æ„Ÿå—ã€‚'},
    'æ†¤æ€’': {'type': 'ç ´å£å‹ / é«˜å¼·åº¦å‹èˆˆè¶£', 'reason': 'ç«æ°£æ»¿æ»¿ï¼Ÿæ‰¾å€‹å®‰å…¨çš„æ–¹å¼æŠŠåŠ›é‡é‡‹æ”¾å‡ºå»ï¼å»åšé«˜å¼·åº¦é‹å‹•ã€æ‰“æ²™åŒ…ï¼Œè®“æƒ…ç·’åœ¨é‹å‹•è£¡è¢«ç‡ƒç‡’æ‰ã€‚'},
    'ç„¦æ…®': {'type': 'å°ˆæ³¨å‹ / é‡è¤‡å‹èˆˆè¶£', 'reason': 'å¿ƒæœ‰é»äº‚ï¼Ÿè©¦è©¦éœ€è¦é‡è¤‡å‹•ä½œçš„å°æ´»å‹•å§ï¼åƒæ˜¯æ‹¼åœ–ã€æ‘ºç´™æˆ–åˆ†é¡å°ç‰©ï¼Œå°ˆæ³¨æ„Ÿæœƒè®“ç„¦æ…®æ…¢æ…¢å®‰éœä¸‹ä¾†ã€‚'},
    'å¹³éœ': {'type': 'æ¢ç´¢å‹ / æ”¾é¬†å‹èˆˆè¶£', 'reason': 'ä½ ç¾åœ¨æ•£ç™¼è‘—ç©©å®šçš„èƒ½é‡ï½ä¸å¦¨æ•£æ­¥æ¢ç´¢å‘¨é­ï¼Œæˆ–è½é»è¼•éŸ³æ¨‚ï¼Œäº«å—é€™ä»½é›£å¾—çš„å¹³å’Œã€‚'},
    'å­æƒ¡': {'type': 'æ¸…ç†å‹ / è½‰æ›å‹èˆˆè¶£', 'reason': 'æœ‰ç¨®è¢«æ±è¥¿æƒ¹æ¯›çš„æ„Ÿè¦ºï¼Ÿé‚£æ­£æ˜¯æ•´ç†çš„å¥½æ™‚æ©Ÿï¼æ¸…æ‰ç…©äººçš„é›œç‰©ï¼Œè®“ç’°å¢ƒå’Œå¿ƒæƒ…ä¸€èµ·ç…¥ç„¶ä¸€æ–°ã€‚'},
    'æœŸå¾…': {'type': 'è¨ˆåŠƒå‹ / å‰µä½œå‹èˆˆè¶£', 'reason': 'ä½ èˆˆå¥®å¾—åƒæº–å‚™é–‹å•Ÿæ–°é—œå¡ï¼è¶é€™è‚¡èƒ½é‡ï¼ŒæŠŠä½ çš„è¨ˆç•«å…·é«”åŒ–å§ï½åˆ—æ¸…å–®ã€æ‰¾éˆæ„Ÿã€åšè…¦åŠ›æ¿€ç›ªï¼Œè®“æœŸå¾…è®Šæˆè¡Œå‹•ã€‚'},
    'é©šè¨': {'type': 'æ¢ç´¢å‹ / èªçŸ¥å‹èˆˆè¶£', 'reason': 'å“‡ï¼ä½ çš„å¥½å¥‡å¿ƒè¢«é»äº®äº†ï¼ä¸å¦‚è¶å‹¢å¤šäº†è§£ä¸€ä¸‹å‰›å‰›è®“ä½ é©šè¨çš„äº‹ï¼ŒæŸ¥è³‡æ–™ã€çœ‹å½±ç‰‡ï¼Œè®“é©šè¨è®Šæˆæœ‰è¶£çš„æ–°ç™¼ç¾ã€‚'},
    'Tokenizer å¤±æ•—': {'type': 'éŒ¯èª¤è¨ºæ–·', 'reason': 'ç”±æ–¼æƒ…ç·’åˆ†ææ¨¡çµ„æœªå•Ÿå‹•ï¼Œç„¡æ³•æä¾›æ¨è–¦ã€‚'},
    'Tokenizer è™•ç†å´©æ½°': {'type': 'éŒ¯èª¤è¨ºæ–·', 'reason': 'æƒ…ç·’åˆ†ææ¨¡çµ„è™•ç†è¼¸å…¥æ™‚å´©æ½°ï¼Œç„¡æ³•æä¾›æ¨è–¦ã€‚'}
}

# 1.3 ç”Ÿæˆå¼æ¨è–¦å‡½å¼ (Gemini çµ„ç¹”èªè¨€èˆ‡éŒ¯èª¤è™•ç†)
def generate_conversational_recommendation(text_input, model, logic, client):
    """çµåˆæƒ…ç·’é æ¸¬çµæœï¼Œå‘¼å« Gemini ç”Ÿæˆå€‹æ€§åŒ–çš„æ•™ç·´å»ºè­°ã€‚"""
    try:
        predicted_emotion, confidence = predict_emotion(text_input, final_model, tokenizer, max_len, emotion_classes)
    except Exception as e:
        return {
            'ai_response': f"LSTM æ¨¡å‹é æ¸¬å¤±æ•—ï¼š{str(e)}",
            'predicted_emotion': "éŒ¯èª¤",
            'confidence': "0.00%"
        }
    
    recommendation_info = logic.get(predicted_emotion, {'type': 'ç„¡æ­¤é¡åˆ¥', 'reason': 'è«‹ä¼‘æ¯'})
    
    # ğŸš¨ å¦‚æœ Tokenizer å¤±æ•—ï¼Œæˆ‘å€‘ä¸å‘¼å« Geminiï¼Œç›´æ¥è¿”å›è¨ºæ–·çµæœ
    if predicted_emotion in ['Tokenizer å¤±æ•—', 'Tokenizer è™•ç†å´©æ½°:']:
        return {
            'ai_response': f"ç³»çµ±éŒ¯èª¤è¨ºæ–·ï¼š{predicted_emotion}ã€‚è«‹æª¢æŸ¥ 'emotion_data.csv' æª”æ¡ˆæ˜¯å¦æ­£ç¢ºæ”¾ç½®æ–¼ä¼ºæœå™¨ã€‚",
            'predicted_emotion': predicted_emotion,
            'confidence': f"{confidence*100:.2f}%"
        }

    
    # ğŸš¨ æœ€çµ‚ä¿®æ­£ï¼šæ¢å¾©å®Œæ•´çš„ Gemini æç¤ºè©
    prompt = f"""
    ä½ æ˜¯ä¸€å€‹æº«æš–ã€å°ˆæ¥­ã€å¹½é»˜çš„ AI å¿ƒç†æ•™ç·´ã€‚ä½ å°é‡æ©Ÿã€é›»å‰ä»–ã€ç¾é£Ÿæ¢ç´¢ç­‰å¤šç¨®èˆˆè¶£æœ‰æ·±åˆ»è¦‹è§£ã€‚
    ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šä»¥ä¸‹è³‡è¨Šï¼Œç”¨è¼•é¬†ä¸”é¼“å‹µçš„èªæ°£çµ¦å‡ºä¸€å€‹**å€‹æ€§åŒ–çš„èˆˆè¶£æ¨è–¦**ï¼Œä¸¦**èå…¥ä¸€åˆ°å…©å€‹ä½ æåˆ°çš„å°ˆæ¥­èˆˆè¶£**ï¼ˆé‡æ©Ÿã€å‰ä»–ã€ç¾é£Ÿç­‰ï¼‰ä½œç‚ºå»ºè­°çš„ä¾‹å­ã€‚
    
    ç”¨æˆ¶åŸå§‹è¼¸å…¥ï¼š"{text_input}"
    æ¨¡å‹é æ¸¬æƒ…ç·’ï¼š{predicted_emotion}
    æƒ…ç·’å»ºè­°é¡å‹ï¼š{recommendation_info['type']}
    å»ºè­°åŸå› ï¼š{recommendation_info['reason']}
    
    è«‹éµå¾ªä»¥ä¸‹æ ¼å¼ï¼š
    1. é–‹é ­å…ˆç”¨ 1-2 å¥è©±æº«æš–åœ°å›æ‡‰ç”¨æˆ¶çš„æƒ…ç·’ã€‚
    2. æ¥è‘—ç”¨ 1-2 å¥è©±èªªæ˜é€™æ˜¯å±¬æ–¼å“ªä¸€ç¨®èˆˆè¶£é¡å‹ï¼ˆä¾‹å¦‚ï¼šã€é€™æ™‚å€™ä½ éœ€è¦çš„æ˜¯ã€Œç™¼æ´©å‹ã€çš„èˆˆè¶£ï¼ã€ï¼‰ã€‚
    3. æœ€å¾Œæå‡ºè‡³å°‘ 3 å€‹å…·é«”çš„èˆˆè¶£å»ºè­°ã€‚
    4. å¿…é ˆç¢ºä¿ä½ çµ¦å‡ºçš„å»ºè­°æ˜¯æœ‰å»ºè¨­æ€§ä¸”æ­£é¢çš„ã€‚
    """
    
    try:
        # å˜—è©¦å‘¼å« Gemini API
        response = client.models.generate_content(
            model='gemini-2.5-pro',
            contents=prompt
        )
        
        return {
            'ai_response': response.text,
            'predicted_emotion': predicted_emotion,
            'confidence': f"{confidence*100:.2f}%"
        }
        
    except APIError as e:
        error_message = str(e)
        if "Permission denied" in error_message or "Invalid API key" in error_message:
             return {
                'ai_response': "Gemini API å‘¼å«å¤±æ•—ï¼šæ¬Šé™é­æ‹’ã€‚è«‹æª¢æŸ¥ Render ä¸Šçš„ GEMINI_API_KEY æ˜¯å¦è¨­å®šæ­£ç¢ºä¸”æœ‰æ•ˆã€‚",
                'predicted_emotion': predicted_emotion,
                'confidence': f"{confidence*100:.2f}%"
            }
        else:
            return {
                'ai_response': f"Gemini API å‘¼å«å¤±æ•—ï¼šç™¼ç”Ÿ API éŒ¯èª¤ {type(e).__name__}ï¼ŒéŒ¯èª¤è¨Šæ¯ï¼š{error_message[:100]}...",
                'predicted_emotion': predicted_emotion,
                'confidence': f"{confidence*100:.2f}%"
            }
    except Exception as e:
        return {
            'ai_response': f"Gemini API å‘¼å«å¤±æ•—ï¼šç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ {type(e).__name__}ï¼Œè«‹æª¢æŸ¥ Render æ—¥èªŒã€‚",
            'predicted_emotion': predicted_emotion,
            'confidence': f"{confidence*100:.2f}%"
        }


# --- 2. Flask API å®šç¾© ---
app = Flask(__name__)
CORS(app) # å•Ÿç”¨ CORS

@app.route('/api/recommend', methods=['POST'])
def recommend():
    # ğŸš¨ å•Ÿå‹•å‰çš„æœ€çµ‚æª¢æŸ¥ï¼šå¦‚æœ Tokenizer æ¨™è¨˜ç‚ºéŒ¯èª¤ï¼Œç›´æ¥è¿”å› 503
    global tokenizer
    if isinstance(tokenizer, str) and tokenizer in ['FILE_NOT_FOUND', 'BUILD_FAILED', 'LIB_FAILED']:
         # 503 Service Unavailable (æœå‹™ä¸å¯ç”¨) æ›´é©åˆæè¿°æœå‹™çš„æ ¸å¿ƒä¾è³´ç¼ºå¤±
         return jsonify({
             "error": "Service Unavailable (503)",
             "message": f"æ ¸å¿ƒä¾è³´éºå¤±æˆ–åˆå§‹åŒ–å¤±æ•—ã€‚Tokenizer ç‹€æ…‹: {tokenizer}ã€‚è«‹ç¢ºèª 'emotion_data.csv' æª”æ¡ˆå·²åœ¨ GitHub å€‰åº«ä¸­ã€‚"
         }), 503
         
    try:
        data = request.get_json()
        user_text = data.get('text', '')
        
        if not user_text:
            return jsonify({"error": "Missing 'text' in request body"}), 400

        # å‘¼å«æ ¸å¿ƒè™•ç†å‡½å¼
        result = generate_conversational_recommendation(user_text, final_model, recommendation_logic, client)
        
        return jsonify(result)

    except Exception as e:
        # æ•ç²æ‰€æœ‰æ„æ–™ä¹‹å¤–çš„åŸ·è¡ŒæœŸéŒ¯èª¤
        print(f"API è™•ç†éŒ¯èª¤: {e}")
        return jsonify({"error": "Internal Server Error", "message": str(e)}), 500
