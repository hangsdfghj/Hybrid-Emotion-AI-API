import os
# é—œéµå„ªåŒ–ï¼šæŠ‘åˆ¶ TensorFlow çš„å•Ÿå‹•æ—¥èªŒå’Œè­¦å‘Š
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
os.environ["CUDA_VISIBLE_DEVICES"] = "-1" # ç¢ºä¿ä½¿ç”¨ CPU

import numpy as np
import jieba
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
from google import genai
from google.genai import types
from flask import Flask, request, jsonify
from flask_cors import CORS

# --- 0. å…¨åŸŸè®Šæ•¸èˆ‡æ¨¡å‹è¼‰å…¥ (åƒ…åœ¨æœå‹™å•Ÿå‹•æ™‚åŸ·è¡Œä¸€æ¬¡) ---

client = genai.Client()

emotion_classes = np.array(['å­æƒ¡', 'å–œæ‚…', 'å¹³éœ', 'æ‚²å‚·', 'æ†¤æ€’', 'æœŸå¾…', 'ç„¦æ…®', 'é©šè¨']) 
max_len = 16 

# ğŸš¨ æœ€çµ‚ä¿®æ­£ï¼šç¹éè€—æ™‚çš„æ¨¡å‹è¼‰å…¥ï¼Œä»¥æ¸¬è©¦æœå‹™æ˜¯å¦èƒ½å•Ÿå‹•
# å› ç‚ºè¼‰å…¥ emotion_model.h5 è¶…é 3 åˆ†é˜æˆ–é€ æˆè¨˜æ†¶é«”å´©æ½°

# å‰µå»ºä¸€å€‹æ¨¡æ“¬æ¨¡å‹ (Mock Model) é¡åˆ¥
class MockEmotionModel:
    """ç”¨æ–¼å–ä»£ TensorFlow æ¨¡å‹ï¼Œè®“æœå‹™å¿«é€Ÿå•Ÿå‹•ä¸¦æ¨¡æ“¬ä¸€å€‹é æ¸¬çµæœ (ä¾‹å¦‚: ç„¦æ…®)ã€‚"""
    def predict(self, padded_sequence, verbose=0):
        # æ¨¡æ“¬ä¸€å€‹ 'ç„¦æ…®' (ç´¢å¼• 6) çš„é«˜ä¿¡å¿ƒåº¦çµæœ
        return np.array([[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.65, 0.05]]) 
# å°‡æœ€çµ‚æ¨¡å‹è¨­ç½®ç‚ºæ¨¡æ“¬æ¨¡å‹
final_model = MockEmotionModel() 
print("æ¨¡å‹è¼‰å…¥å·²æ—è·¯ã€‚æ­£åœ¨ä½¿ç”¨æ¨¡æ“¬æ¨¡å‹é€²è¡Œå•Ÿå‹•æ¸¬è©¦ã€‚")


# ä¿®æ­£å¾Œçš„ Tokenizer é‡å»ºé‚è¼¯ (é€™éƒ¨åˆ†å¿…é ˆæˆåŠŸï¼Œå› ç‚ºæ˜¯ç´” Python æ“ä½œ)
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer

print("--- æ­£åœ¨é‡å»º Tokenizer... ---")
try:
    # è¼‰å…¥æ•¸æ“šç”¨æ–¼é‡å»º Tokenizer
    df = pd.read_csv('emotion_data.csv', header=None, names=['text', 'emotion'])
    
    # é‡æ–°åŸ·è¡Œåˆ†è©èˆ‡å»ºç«‹
    df['tokens'] = df['text'].apply(lambda x: list(jieba.cut(x, cut_all=False)))
    texts = [" ".join(tokens) for tokens in df['tokens']]
    
    # é‡æ–°å»ºç«‹ Tokenizer è®Šæ•¸
    tokenizer = Tokenizer(num_words=5000, oov_token="<unk>") 
    tokenizer.fit_on_texts(texts)
    
    print("'tokenizer' å·²æˆåŠŸå¾ emotion_data.csv é‡å»ºï¼")

except FileNotFoundError:
    print("FATAL ERROR: ç„¡æ³•æ‰¾åˆ° 'emotion_data.csv' æª”æ¡ˆã€‚è«‹ç¢ºä¿å®ƒåœ¨ app.py çš„åŒä¸€å€‹è³‡æ–™å¤¾ä¸­ï¼")

# --- 1. æ ¸å¿ƒé‚è¼¯å‡½å¼ ---

# 1.1 LSTM åˆ¤æ–·æƒ…ç·’ (ç¾åœ¨æœƒä½¿ç”¨ MockModel.predict)
def predict_emotion(text_input, model, tokenizer, max_len, emotion_classes):
    """ä½¿ç”¨ Mock æ¨¡å‹é æ¸¬è¼¸å…¥æ–‡æœ¬çš„æƒ…ç·’ï¼Œä»¥ç¹é TF è¼‰å…¥å•é¡Œã€‚"""
    # é€™è£¡çš„åˆ†è©å’Œ Padding ä»ç„¶æ˜¯å¿…è¦çš„æ­¥é©Ÿ
    tokens = list(jieba.cut(text_input, cut_all=False))
    text_processed = [" ".join(tokens)]
    sequence = tokenizer.texts_to_sequences(text_processed)
    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post', truncating='post')
    
    # å‘¼å« MockModel.predictï¼Œå®ƒæœƒå›å‚³æˆ‘å€‘é è¨­çš„ 'ç„¦æ…®' çµæœ
    predictions = model.predict(padded_sequence, verbose=0)
    
    predicted_class = np.argmax(predictions, axis=1)[0]
    predicted_emotion = emotion_classes[predicted_class]
    confidence = predictions[0][predicted_class]
    
    return predicted_emotion, confidence

# 1.2 æ¨è–¦é‚è¼¯ (ä¿æŒä¸è®Š)
recommendation_logic = {
    'å–œæ‚…': {'type': 'ç¤¾äº¤å‹èˆˆè¶£ / åˆ†äº«', 'reason': 'ä½ å¿ƒæƒ…è¶…æ£’ï¼æ˜¯æ™‚å€™è·Ÿæœ‹å‹åˆ†äº«é€™ä»½å–œæ‚…ï¼Œèˆ‰è¾¦ä¸€å ´ç¾é£Ÿèšæœƒå§ï¼'},
    'æ‚²å‚·': {'type': 'å‰µé€ å‹ / ç™¼æ´©å‹èˆˆè¶£', 'reason': 'ä½ ç¾åœ¨çš„æƒ…ç·’éœ€è¦å‡ºå£ï¼Œä¸å¦‚æ‹¿èµ·ç´™ç­†ç•«ä¸‹ä½ çš„å¿ƒæƒ…ï¼Œæˆ–å¯«é»æ±è¥¿å§ï¼è®“å‰µä½œæ›¿ä½ èªªå‡ºé‚£äº›èªªä¸å‡ºå£çš„æ„Ÿå—ã€‚'},
    'æ†¤æ€’': {'type': 'ç ´å£å‹ / é«˜å¼·åº¦å‹èˆˆè¶£', 'reason': 'ç«æ°£æ»¿æ»¿ï¼Ÿæ‰¾å€‹å®‰å…¨çš„æ–¹å¼æŠŠåŠ›é‡é‡‹æ”¾å‡ºå»ï¼å»åšé«˜å¼·åº¦é‹å‹•ã€æ‰“æ²™åŒ…ï¼Œè®“æƒ…ç·’åœ¨é‹å‹•è£¡è¢«ç‡ƒç‡’æ‰ã€‚'},
    'ç„¦æ…®': {'type': 'å°ˆæ³¨å‹ / é‡è¤‡å‹èˆˆè¶£', 'reason': 'å¿ƒæœ‰é»äº‚ï¼Ÿè©¦è©¦éœ€è¦é‡è¤‡å‹•ä½œçš„å°æ´»å‹•å§ï¼åƒæ˜¯æ‹¼åœ–ã€æ‘ºç´™æˆ–åˆ†é¡å°ç‰©ï¼Œå°ˆæ³¨æ„Ÿæœƒè®“ç„¦æ…®æ…¢æ…¢å®‰éœä¸‹ä¾†ã€‚'},
    'å¹³éœ': {'type': 'æ¢ç´¢å‹ / æ”¾é¬†å‹èˆˆè¶£', 'reason': 'ä½ ç¾åœ¨æ•£ç™¼è‘—ç©©å®šçš„èƒ½é‡ï½ä¸å¦¨æ•£æ­¥æ¢ç´¢å‘¨é­ï¼Œæˆ–è½é»è¼•éŸ³æ¨‚ï¼Œäº«å—é€™ä»½é›£å¾—çš„å¹³å’Œã€‚'},
    'å­æƒ¡': {'type': 'æ¸…ç†å‹ / è½‰æ›å‹èˆˆè¶£', 'reason': 'æœ‰ç¨®è¢«æ±è¥¿æƒ¹æ¯›çš„æ„Ÿè¦ºï¼Ÿé‚£æ­£æ˜¯æ•´ç†çš„å¥½æ™‚æ©Ÿï¼æ¸…æ‰ç…©äººçš„é›œç‰©ï¼Œè®“ç’°å¢ƒå’Œå¿ƒæƒ…ä¸€èµ·ç…¥ç„¶ä¸€æ–°ã€‚'},
    'æœŸå¾…': {'type': 'è¨ˆåŠƒå‹ / å‰µä½œå‹èˆˆè¶£', 'reason': 'ä½ èˆˆå¥®å¾—åƒæº–å‚™é–‹å•Ÿæ–°é—œå¡ï¼è¶é€™è‚¡èƒ½é‡ï¼ŒæŠŠä½ çš„è¨ˆç•«å…·é«”åŒ–å§ï½åˆ—æ¸…å–®ã€æ‰¾éˆæ„Ÿã€åšè…¦åŠ›æ¿€ç›ªï¼Œè®“æœŸå¾…è®Šæˆè¡Œå‹•ã€‚'},
    'é©šè¨': {'type': 'æ¢ç´¢å‹ / èªçŸ¥å‹èˆˆè¶£', 'reason': 'å“‡ï¼ä½ çš„å¥½å¥‡å¿ƒè¢«é»äº®äº†ï¼ä¸å¦‚è¶å‹¢å¤šäº†è§£ä¸€ä¸‹å‰›å‰›è®“ä½ é©šè¨çš„äº‹ï¼ŒæŸ¥è³‡æ–™ã€çœ‹å½±ç‰‡ï¼Œè®“é©šè¨è®Šæˆæœ‰è¶£çš„æ–°ç™¼ç¾ã€‚'}
}

# 1.3 ç”Ÿæˆå¼æ¨è–¦å‡½å¼ (Gemini çµ„ç¹”èªè¨€èˆ‡éŒ¯èª¤è™•ç†)
def generate_conversational_recommendation(text_input, model, logic, client):
    """çµåˆæƒ…ç·’é æ¸¬çµæœï¼Œå‘¼å« Gemini ç”Ÿæˆå€‹æ€§åŒ–çš„æ•™ç·´å»ºè­°ã€‚"""
    try:
        # å³ä½¿æ˜¯ Mock æ¨¡å‹ï¼Œæˆ‘å€‘ä¹Ÿéœ€è¦é‹è¡Œ predict_emotion
        predicted_emotion, confidence = predict_emotion(text_input, final_model, tokenizer, max_len, emotion_classes)
    except Exception as e:
        return {
            'ai_response': f"LSTM æ¨¡å‹é æ¸¬å¤±æ•—ï¼š{str(e)}",
            'predicted_emotion': "éŒ¯èª¤",
            'confidence': "0.00%"
        }
    
    recommendation_info = logic.get(predicted_emotion, {'type': 'ç„¡æ­¤é¡åˆ¥', 'reason': 'è«‹ä¼‘æ¯'})
    
    prompt = f"""
    ä½ æ˜¯ä¸€å€‹æº«æš–ã€å°ˆæ¥­ã€å¹½é»˜çš„ AI å¿ƒç†æ•™ç·´ã€‚ä½ å°é‡æ©Ÿã€é›»å‰ä»–ã€ç¾é£Ÿæ¢ç´¢ç­‰å¤šç¨®èˆˆè¶£æœ‰æ·±åˆ»è¦‹è§£ã€‚
    ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šä»¥ä¸‹è³‡è¨Šï¼Œç”¨è¦ªåˆ‡çš„å£èªåŒ–èªæ°£ï¼Œé¼“å‹µç”¨æˆ¶ä¸¦çµ¦äºˆä¸€å€‹å…·é«”çš„ã€èˆ‡ä»–å€‘èˆˆè¶£ï¼ˆé‡æ©Ÿã€é›»å‰ä»–ã€ç¾é£Ÿï¼‰ç›¸é—œçš„è¡Œå‹•å»ºè­°ã€‚
    è«‹é¿å…ä½¿ç”¨å›ºå®šçš„æ¨¡æ¿ã€‚

    ç”¨æˆ¶çš„åŸå§‹è¼¸å…¥æ˜¯: "{text_input}"
    ç³»çµ±åˆ¤æ–·çš„æƒ…ç·’æ˜¯: {predicted_emotion}
    ç³»çµ±å»ºè­°çš„æ´»å‹•é¡å‹æ˜¯: {recommendation_info['type']}
    ç³»çµ±å»ºè­°çš„å›ºå®šç†ç”±æ˜¯: {recommendation_info['reason']}
    
    è«‹æ ¹æ“šé€™äº›è³‡è¨Šï¼Œç”Ÿæˆä¸€æ®µæµæš¢çš„é¼“å‹µå’Œæ¨è–¦èªã€‚
    """
    
    try:
        # å˜—è©¦å‘¼å« Gemini API
        response = client.models.generate_content(
            model='gemini-2.5-pro',
            contents=prompt
        )
        
        return {
            'ai_response': response.text,
            'predicted_emotion': predicted_emotion,
            'confidence': f"{confidence*100:.2f}%"
        }
        
    except genai.errors.PermissionDeniedError:
        # å°ˆé–€è™•ç† API Key éŒ¯èª¤ (403)
        return {
            'ai_response': "Gemini API å‘¼å«å¤±æ•—ï¼šæ¬Šé™é­æ‹’ã€‚è«‹æª¢æŸ¥ Render ä¸Šçš„ GEMINI_API_KEY æ˜¯å¦è¨­å®šæ­£ç¢ºä¸”æœ‰æ•ˆã€‚",
            'predicted_emotion': predicted_emotion,
            'confidence': f"{confidence*100:.2f}%"
        }
    except Exception as e:
        # è™•ç†å…¶ä»–æ‰€æœ‰ API éŒ¯èª¤ (å¦‚ 400, 500, Timeout)
        return {
            'ai_response': f"Gemini API å‘¼å«å¤±æ•—ï¼šç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ {type(e).__name__}ï¼Œè«‹æª¢æŸ¥ Render æ—¥èªŒã€‚",
            'predicted_emotion': predicted_emotion,
            'confidence': f"{confidence*100:.2f}%"
        }


# --- 2. Flask API å®šç¾© ---
app = Flask(__name__)
CORS(app) # å•Ÿç”¨ CORS

@app.route('/api/recommend', methods=['POST'])
def recommend():
    try:
        data = request.get_json()
        user_text = data.get('text', '')
        
        if not user_text:
            return jsonify({"error": "Missing 'text' in request body"}), 400

        # å‘¼å«æ ¸å¿ƒè™•ç†å‡½å¼
        result = generate_conversational_recommendation(user_text, final_model, recommendation_logic, client)
        
        return jsonify(result)

    except Exception as e:
        print(f"API è™•ç†éŒ¯èª¤: {e}")
        return jsonify({"error": "Internal Server Error", "message": str(e)}), 500
